
@inproceedings{linderman_bayesian_2017,
	title = {Bayesian {Learning} and {Inference} in {Recurrent} {Switching} {Linear} {Dynamical} {Systems}},
	url = {https://proceedings.mlr.press/v54/linderman17a.html},
	abstract = {Many natural systems, such as neurons firing in the brain or basketball teams traversing a court, give rise to time series data with complex, nonlinear dynamics.  We can gain insight into these systems by decomposing the data into segments that are each explained by simpler dynamic units. Building on switching linear dynamical systems (SLDS), we develop a model class and Bayesian inference algorithms that not only discover these dynamical units but also, by learning how transition probabilities depend on observations or continuous latent states, explain their switching behavior.  Our key innovation is to design these recurrent SLDS models to enable recent Pólya-gamma auxiliary variable techniques and thus make approximate Bayesian learning and inference in these models easy, fast, and scalable.},
	language = {en},
	urldate = {2023-05-16},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Linderman, Scott and Johnson, Matthew and Miller, Andrew and Adams, Ryan and Blei, David and Paninski, Liam},
	month = apr,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {914--922},
	file = {Full Text PDF:C\:\\Users\\Rohin Gilman\\Zotero\\storage\\X3VYD4XC\\Linderman et al. - 2017 - Bayesian Learning and Inference in Recurrent Switc.pdf:application/pdf;Supplementary PDF:C\:\\Users\\Rohin Gilman\\Zotero\\storage\\MWR6E6Q9\\Linderman et al. - 2017 - Bayesian Learning and Inference in Recurrent Switc.pdf:application/pdf},
}

@misc{zoltowski_unifying_2020,
	title = {Unifying and generalizing models of neural dynamics during decision-making},
	url = {http://arxiv.org/abs/2001.04571},
	doi = {10.48550/arXiv.2001.04571},
	abstract = {An open question in systems and computational neuroscience is how neural circuits accumulate evidence towards a decision. Fitting models of decision-making theory to neural activity helps answer this question, but current approaches limit the number of these models that we can fit to neural data. Here we propose a unifying framework for modeling neural activity during decision-making tasks. The framework includes the canonical drift-diffusion model and enables extensions such as multi-dimensional accumulators, variable and collapsing boundaries, and discrete jumps. Our framework is based on constraining the parameters of recurrent state-space models, for which we introduce a scalable variational Laplace-EM inference algorithm. We applied the modeling approach to spiking responses recorded from monkey parietal cortex during two decision-making tasks. We found that a two-dimensional accumulator better captured the trial-averaged responses of a set of parietal neurons than a single accumulator model. Next, we identified a variable lower boundary in the responses of an LIP neuron during a random dot motion task.},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Zoltowski, David M. and Pillow, Jonathan W. and Linderman, Scott W.},
	month = jan,
	year = {2020},
	note = {arXiv:2001.04571 [q-bio, stat]},
	keywords = {Statistics - Machine Learning, Quantitative Biology - Neurons and Cognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Rohin Gilman\\Zotero\\storage\\MYJ7BDAK\\Zoltowski et al. - 2020 - Unifying and generalizing models of neural dynamic.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Rohin Gilman\\Zotero\\storage\\ZIQNM5AL\\2001.html:text/html},
}

@inproceedings{chen_estimating_2015,
	title = {Estimating state and parameters in state space models of spike trains},
	isbn = {978-1-107-07919-9 978-1-139-94143-3},
	url = {https://www.cambridge.org/core/product/identifier/CBO9781139941433A054/type/book_part},
	doi = {10.1017/CBO9781139941433.007},
	abstract = {Neural computations at all scales of evolutionary and behavioural complexity are carried out by recurrently connected networks of neurons that communicate with each other, with neurons elsewhere in the brain, and with muscles through the firing of action potentials or “spikes”. To understand how nervous tissue computes, it is therefore necessary to understand how the spiking of neurons is shaped both by inputs to the network and by the recurrent action of existing network activity. Whereas most historical spike data were collected one neuron at a time, new techniques including silicon multi-electrode array recording and scanning 2-photon, light-sheet or light-field fluorescence calcium imaging increasingly make it possible to record spikes from dozens, hundreds and potentially thousands of individual neurons simultaneously. These new data offer unprecedented empirical access to network computation, promising breakthroughs both in our understanding of neural coding and computation (Stevenson \& Kording 2011), and our ability to build prosthetic neural interfaces (Santhanam, Ryu, Yu, Afshar \& Shenoy 2006). Fulfilment of this promise will require powerful methods for data modelling and analysis, able to capture},
	urldate = {2023-05-16},
	publisher = {Cambridge University Press},
	author = {MacKe, J. H. and Buesing, L. and Sahani, M.},
	editor = {Chen, Zhe},
	month = sep,
	year = {2015},
	doi = {10.1017/CBO9781139941433.007},
	note = {Book Title: Advanced State Space Methods for Neural and Clinical Data
Edition: 1},
	pages = {137--159},
}

@article{blei_variational_2017,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	volume = {112},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Variational {Inference}},
	url = {http://arxiv.org/abs/1601.00670},
	doi = {10.1080/01621459.2017.1285773},
	abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
	number = {518},
	urldate = {2023-05-16},
	journal = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = apr,
	year = {2017},
	note = {arXiv:1601.00670 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation},
	pages = {859--877},
	file = {arXiv Fulltext PDF:C\:\\Users\\Rohin Gilman\\Zotero\\storage\\S9743W6A\\Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Rohin Gilman\\Zotero\\storage\\L2HE8WV8\\1601.html:text/html},
}

@inproceedings{linderman_dependent_2015,
	title = {Dependent {Multinomial} {Models} {Made} {Easy}: {Stick}-{Breaking} with the {Polya}-gamma {Augmentation}},
	volume = {28},
	shorttitle = {Dependent {Multinomial} {Models} {Made} {Easy}},
	url = {https://papers.nips.cc/paper_files/paper/2015/hash/07a4e20a7bbeeb7a736682b26b16ebe8-Abstract.html},
	abstract = {Many practical modeling problems involve discrete data that are best represented as draws from multinomial or categorical distributions. For example, nucleotides in a DNA sequence, children's names in a given state and year, and text documents are all commonly modeled with multinomial distributions.  In all of these cases, we expect some form of dependency between the draws: the nucleotide at one position in the DNA strand may depend on the preceding nucleotides, children's names are highly correlated from year to year, and topics in text may be correlated and dynamic.  These dependencies are not naturally captured by the typical Dirichlet-multinomial formulation.  Here, we leverage a logistic stick-breaking representation and recent innovations in P{\textbackslash}'\{o\}lya-gamma augmentation to reformulate the multinomial distribution in terms of latent variables with jointly Gaussian likelihoods, enabling us to take advantage of a host of Bayesian inference techniques for Gaussian models with minimal overhead.},
	urldate = {2023-05-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Linderman, Scott and Johnson, Matthew J and Adams, Ryan P},
	year = {2015},
	file = {Full Text PDF:C\:\\Users\\Rohin Gilman\\Zotero\\storage\\8LY7JSGP\\Linderman et al. - 2015 - Dependent Multinomial Models Made Easy Stick-Brea.pdf:application/pdf},
}
